{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c0f035",
   "metadata": {},
   "source": [
    "## Podstawy Uczenia Maszynowego – Projekt\n",
    "### Rozpoznawanie nadużyć w transakcjach płatniczych na podstawie fizycznych okoliczności ich wykonania\n",
    "#### Autorzy: Kacper Korecki, Dominik Zakrzewski, Konrad Zbylut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57969e07",
   "metadata": {},
   "source": [
    "## Wprowadzenie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeecbc7",
   "metadata": {},
   "source": [
    "Celem naszego projektu jest analiza obszernego zbioru transakcji płatniczych za pomocą kilku wybranych algorytmów dla porównania ich charakterystyki i otrzymanych wyników."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9812a1",
   "metadata": {},
   "source": [
    "## O zbiorze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0c15b",
   "metadata": {},
   "source": [
    "Prezentowany przez nas zbiór danych \"card transaction data\" jest zbiorem przedstawiającym transakcje dokonywane kartami płatniczymi. Ilość rekordów w datasecie wynosi 1 000 000.\n",
    "\n",
    "Argumenty:\n",
    "* `distance_from_home` – odległość miejsca wykonania transakcji od adresu domowego klienta banku,\n",
    "* `distance_from_last_transaction` – odległość miejsca wykonania transakcji od miejsca poprzedniej transakcji,\n",
    "* `ratio_to_median_purchase_price` – stosunek wysokości kwoty transakcji do mediany wszystkich transakcji danego klienta,\n",
    "* `repeat_retailer` – argument logiczny (0.0/1.0) mówiący o tym, czy dany klient wykonywał już transakcję u danego sprzedającego,\n",
    "* `used_chip` – argument logiczny (0.0/1.0) mówiący o tym, czy przy transakcji została użyta karta (tj. czy faktycznie plastikowa karta została włożona np. do terminalu)\n",
    "* `used_pin_number` – argument logiczny (0.0/1.0) mówiący o tym, czy podczas transakcji został podany numer PIN,\n",
    "* `online_order` – argument logiczny (0.0/1.0) mówiący o tym, czy transakcja została wykonana przez Internet,\n",
    "\n",
    "Wynik:\n",
    "* `fraud` – argument logiczny (0.0/1.0) mówiący o tym, czy transakcja była nadużyciem (czyli np. została wykonana przez osobę, która włamała się na konto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5047a",
   "metadata": {},
   "source": [
    "## Wczytanie i obróbka zbioru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42123dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b879d7",
   "metadata": {},
   "source": [
    "Wczytujemy zbiór danych z pliku CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ab979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('card_transdata.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d62f5e",
   "metadata": {},
   "source": [
    "Ponadto sprawdzamy stosunek transakcji oznaczonych jako nadużycia (y=1.0) do liczby wszystkich transakcji w zbiorze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee87628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200964f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fraud_transactions = np.sum(data_np[:, 7])\n",
    "print(\"Percentage of fraud transactions:\", total_fraud_transactions / data_np.shape[0] * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b1be5",
   "metadata": {},
   "source": [
    "Jak widzimy różnorodność jest dla nas niezadowalająco niska, gdyż większość transakcji stanowią \"normalne\" transakcje. Ponadto liczba rekordów jest bardzo duża, więc na potrzeby prezentacji i ze względu na złożoność niektórych algorytmów (np. SVM) musimy zmniejszyć nieco liczbę rekordów. Stąd też mamy potrzebę dostosowania zbioru tak, by lepiej trenowały się na nim modele. W tym celu zbudujemy zbiory `X`,`y` w taki sposób, by liczba transakcji \"normalnych\" była ograniczona tj. podobna do liczby transakcji oznaczonych jako nadużycia, a więc łącznie dostaniemy 160 000 transakcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b885c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = data_np[:, :-1]\n",
    "y_raw = data_np[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fe8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# X_raw = StandardScaler().fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20708d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "non_fraud_limit = 85000\n",
    "non_fraud_c = 0\n",
    "\n",
    "for i in range(len(X_raw)):\n",
    "    if y_raw[i] == 0.0 and non_fraud_c <= non_fraud_limit:\n",
    "        X.append(X_raw[i])\n",
    "        y.append(y_raw[i])\n",
    "        non_fraud_c += 1\n",
    "    elif y_raw[i] == 1.0:\n",
    "        X.append(X_raw[i])\n",
    "        y.append(y_raw[i])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5af75d",
   "metadata": {},
   "source": [
    "Dokonujemy podziału na zbiory X i y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "def confusion_matrix_from_preds(model, title):\n",
    "    preds = model.predict(X_test)\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "    cm.ax_.set_title(title)\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258f7d8",
   "metadata": {},
   "source": [
    "## Naiwny klasyfikator Bayesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c677bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb_model = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "gaussian_nb_train_sc = gaussian_nb_model.score(X_train, y_train)\n",
    "gaussian_nb_test_sc = gaussian_nb_model.score(X_test, y_test)\n",
    "\n",
    "confusion_matrix_from_preds(gaussian_nb_model, 'Gaussian NB Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "complement_nb_model = ComplementNB().fit(X_train, y_train)\n",
    "\n",
    "complement_nb_train_sc = complement_nb_model.score(X_train, y_train)\n",
    "complement_nb_test_sc = complement_nb_model.score(X_test, y_test)\n",
    "\n",
    "confusion_matrix_from_preds(complement_nb_model, 'Complement NB Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "bernoulli_nb_model = BernoulliNB().fit(X_train, y_train)\n",
    "\n",
    "bernoulli_nb_train_sc = bernoulli_nb_model.score(X_train, y_train)\n",
    "bernoulli_nb_test_sc = bernoulli_nb_model.score(X_test, y_test)\n",
    "\n",
    "confusion_matrix_from_preds(bernoulli_nb_model, 'Bernoulli NB Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb_model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "multinomial_nb_train_sc = multinomial_nb_model.score(X_train, y_train)\n",
    "multinomial_nb_test_sc = multinomial_nb_model.score(X_test, y_test)\n",
    "\n",
    "confusion_matrix_from_preds(multinomial_nb_model, 'Multinomial NB Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2bc394",
   "metadata": {},
   "source": [
    "## Drzewa decyzyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = tree.DecisionTreeClassifier(random_state=1)\n",
    "dt_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dt_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = path.ccp_alphas\n",
    "print(dt_classifier.score(X_test, y_test))\n",
    "print(dt_classifier.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "depths = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    classifier = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=alpha)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    classifiers.append(classifier)\n",
    "    depths.append(classifier.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cd679",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in classifiers]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c715ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = classifiers[:-1]\n",
    "depths = depths[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f48bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Depths')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.plot(depths, train_scores[:-1], marker='o', label='train')\n",
    "ax.plot(depths, test_scores[:-1], marker='o', label='test')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_from_preds(dt_classifier, 'Decision Tree Classifier Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ded398",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(kernel='rbf', verbose=True).fit(X_train, y_train)\n",
    "preds = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf75285",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_from_preds(svm_model, 'SVM Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41ae5c",
   "metadata": {},
   "source": [
    "### SVM with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9194995",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC( verbose=True)\n",
    "params = {\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'C': [1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(svm_model, params, cv=3, verbose=2)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecf0de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix_from_preds(rs, 'SVM with RS Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Najlepsze parametry: {rs.best_params_}')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
